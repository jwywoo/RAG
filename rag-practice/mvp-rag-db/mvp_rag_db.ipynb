{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Env Setting\n",
        "- Python Virtual Env\n",
        "  - .mvp_rag_db_env"
      ],
      "metadata": {
        "id": "wkrYG3gddnsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dK6OCcp9c6-d"
      },
      "outputs": [],
      "source": [
        "# importing google drive for the session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a virtual env just once\n",
        "# %cd /content/drive/MyDrive\n",
        "# !apt install python3.10-venv\n",
        "# !python3 -m venv .mvp_rag_db_env"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IGWTEyvgfTcU"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the env\n",
        "!source /content/drive/MyDrive/.mvp_rag_db_env/bin/activate"
      ],
      "metadata": {
        "id": "SziovEVOf0gd"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Installation\n",
        "- torch\n",
        "- transformers\n",
        "- faiss-cpu\n",
        "- numpy\n",
        "- pandas\n",
        "- langchain\n",
        "- sqlalchemy"
      ],
      "metadata": {
        "id": "7u4zksYdf7qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers faiss-cpu numpy pandas langchain sqlalchemy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mfgOwuHogCpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON Prep\n",
        "1. Load Json"
      ],
      "metadata": {
        "id": "b9t9AFVNpMxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Json\n",
        "import json\n",
        "import os\n",
        "\n",
        "JSON_DIR_PATH = '/content/drive/MyDrive/JSON'\n",
        "HA_RAG_DATA_DIR_PATH = os.path.join(JSON_DIR_PATH, 'HA_RAG_DATA')\n",
        "\n",
        "json_files = os.listdir(HA_RAG_DATA_DIR_PATH)\n",
        "\n",
        "current_json_file = os.path.join(HA_RAG_DATA_DIR_PATH,json_files[0])\n",
        "\n",
        "current_data = None\n",
        "if (os.path.exists(current_json_file)):\n",
        "    with open(current_json_file, 'r') as f:\n",
        "        current_data = json.load(f)\n",
        "else:\n",
        "  print(f\"Can't find {json_files[0]}\")"
      ],
      "metadata": {
        "id": "qcsQs2-Epv12"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing & Embedding: KoBert\n",
        "1. Load KoBert: Embedding Model and Tokenizer\n",
        "2. Json to Vectors\n",
        "  - Tokenization: json to tokenized text\n",
        "  - Embeddings Generation: tokenized json to vector"
      ],
      "metadata": {
        "id": "dnhoRyVKpUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBert Tokenization Installed\n",
        "# !pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ch3a39ouWNES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load KoBert: Embedding Model and Tokenizer\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')"
      ],
      "metadata": {
        "id": "fRP0yLFa0dbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Json to Vectors\n",
        "# Tokenization: json to tokenized text\n",
        "fields = [\n",
        "    \"ad_gu\",\n",
        "    \"ad_dong\",\n",
        "    \"address\",\n",
        "    \"location\",\n",
        "    \"description\",\n",
        "    \"rating\",\n",
        "    \"share_link\",\n",
        "    \"reviews\",\n",
        "    \"info\"\n",
        "]\n",
        "current_data_meta = current_data[0]\n",
        "\n",
        "text = f\"{current_data_meta['ad_gu']} [SEP] {current_data_meta['ad_dong']} [SEP] {current_data_meta['address']} [SEP] {current_data_meta['location']} [SEP] \" \\\n",
        "           f\"{current_data_meta['description']} [SEP] {current_data_meta['rating']} [SEP] {current_data_meta['share_link']} [SEP] \" \\\n",
        "           f\"{' '.join(current_data_meta['reviews'])} [SEP] {current_data_meta['info']}\"\n",
        "\n",
        "text_clean = text.replace('\\n', ' ')\n",
        "\n",
        "# Tokenize the text\n",
        "inputs = tokenizer(text, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "# Embeddings Generation: tokenized json to vector\n",
        "with torch.no_grad():\n",
        "  outputs = model(**inputs)\n",
        "  embedding = outputs.last_hidden_state[:, 0, :]\n",
        "  embedding = embedding.squeeze().numpy()\n"
      ],
      "metadata": {
        "id": "5G03pXPi1wJl"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS for Vector Search\n",
        "- FAISS creating an index for vectors\n",
        "- RDBMS metadata -> not now"
      ],
      "metadata": {
        "id": "APf6PeRcpfIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "flatten_embedding = np.array(embedding).astype('float32')\n",
        "\n",
        "if flatten_embedding.ndim == 1:\n",
        "    flatten_embedding = flatten_embedding.reshape(1, -1)\n",
        "\n",
        "embedding_index = faiss.IndexFlatL2(flatten_embedding.shape[1])\n",
        "embedding_index.add(flatten_embedding)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zq7HTpDEff8d"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with LangChain"
      ],
      "metadata": {
        "id": "_m5S72f2pkfz"
      }
    }
  ]
}