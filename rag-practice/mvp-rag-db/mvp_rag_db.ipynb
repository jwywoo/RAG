{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "wkrYG3gddnsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK6OCcp9c6-d",
        "outputId": "d8291ba5-b21d-4cc8-bf4d-f4748e5f6f47"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# importing google drive for the session\n",
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Installation\n",
        "- torch\n",
        "- transformers\n",
        "- faiss-cpu\n",
        "- numpy\n",
        "- langchain\n",
        "- pandas pymongo"
      ],
      "metadata": {
        "id": "7u4zksYdf7qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install torch transformers faiss-cpu numpy pandas langchain sqlalchemy pymongo"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mfgOwuHogCpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON Prep\n",
        "1. Load Json\n",
        "  - `json_dir(dir_path)`: Creating list of file path to json in a given directory\n",
        "  - `json_load(file_path)`: Getting JSON of given file path"
      ],
      "metadata": {
        "id": "b9t9AFVNpMxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Json\n",
        "import json\n",
        "import os\n",
        "\n",
        "def json_dir(dir_path):\n",
        "  json_files = os.listdir(dir_path)\n",
        "  json_files_path = [os.path.join(dir_path, file) for file in json_files]\n",
        "  return json_files_path\n",
        "\n",
        "\n",
        "def json_load(file_path):\n",
        "  current_data = None\n",
        "  if (os.path.exists(file_path)):\n",
        "    with open(file_path, 'r') as f:\n",
        "      current_data = json.load(f)\n",
        "  else:\n",
        "    print(f\"Can't find {file_path}\")\n",
        "  return current_data"
      ],
      "metadata": {
        "id": "qcsQs2-Epv12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing & Embedding: KoBert\n",
        "1. Load KoBert: Embedding Model and Tokenizer\n",
        "2. Json to Vectors\n",
        "  - Tokenization: json to tokenized text\n",
        "    - `json_tokenization(data, fields, tokenizer)`: Creating a list of tokenized json data by list of json data\n",
        "  - Embeddings Generation: tokenized json to vector\n",
        "    - `tokens_to_vectors(inputs, model)`: Creating a list of embeddings by a list of tokenized json"
      ],
      "metadata": {
        "id": "dnhoRyVKpUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBert Tokenization Installed\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ch3a39ouWNES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load KoBert: Embedding Model and Tokenizer\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1').to('cuda')\n",
        "model_cpu = BertModel.from_pretrained('skt/kobert-base-v1')\n"
      ],
      "metadata": {
        "id": "fRP0yLFa0dbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Json to vectors\n",
        "# Tokenizations(Json -> Tokenized Json)\n",
        "def json_tokenization(data, tokenizer):\n",
        "  inputs = []\n",
        "  for row in data:\n",
        "      text = f\"{row['ad_gu']} [SEP] {row['ad_dong']} [SEP] {row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "               f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "               f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "      text_clean = text.replace('\\n', ' ')\n",
        "      # Tokenize and move input tensors to GPU\n",
        "      tokenized_input = tokenizer(text_clean, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "      tokenized_input = {key: value.to('cuda') for key, value in tokenized_input.items()}  # Move inputs to GPU\n",
        "      inputs.append(tokenized_input)\n",
        "  return inputs\n",
        "\n",
        "def json_tokenization_cpu(data, tokenizer):\n",
        "  inputs = []\n",
        "  for row in data:\n",
        "      text = f\"{row['ad_gu']} [SEP] {row['ad_dong']} [SEP] {row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "               f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "               f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "      text_clean = text.replace('\\n', ' ')\n",
        "      tokenized_input = tokenizer(text_clean, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "      inputs.append(tokenized_input)\n",
        "  return inputs"
      ],
      "metadata": {
        "id": "BuI1Nx7zfmoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Json to vectors\n",
        "# Embeddings Generation: tokenized json to vector\n",
        "def tokens_to_vectors(inputs, model):\n",
        "  model.to('cuda')\n",
        "  embeddings = []\n",
        "  for input in inputs:\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**input)\n",
        "      embedding = outputs.last_hidden_state[:, 0, :]\n",
        "      embedding = embedding.squeeze().cpu().numpy()\n",
        "      embeddings.append(embedding)\n",
        "  return embeddings\n",
        "\n",
        "def tokens_to_vectors_cpu(inputs, model):\n",
        "  embeddings = []\n",
        "  for input in inputs:\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**input)\n",
        "      embedding = outputs.last_hidden_state[:, 0, :]\n",
        "      embedding = embedding.squeeze().numpy()\n",
        "      embeddings.append(embedding)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "iECMKa5Efs-A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS for Vector Search\n",
        "- FAISS creating an index for vectors\n",
        "  - `faiss_index_gen(embeddings, index_dir_path)`: Creating a index file of given embeddings\n",
        "- RDBMS metadata -> not now"
      ],
      "metadata": {
        "id": "APf6PeRcpfIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "def faiss_index_gen(embeddings, index_dir_path, embeddings_dir_path):\n",
        "  embedding_dim = len(embeddings[0])\n",
        "  index = faiss.IndexFlatL2(embedding_dim)\n",
        "  embeddings_array = np.array(embeddings).astype('float32')\n",
        "  index.add(embeddings_array)\n",
        "  faiss.write_index(index, index_dir_path)\n",
        "  np.save(embeddings_dir_path, embeddings)\n",
        "  return index"
      ],
      "metadata": {
        "id": "zq7HTpDEff8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Json to Vector DB\n",
        "- `json_dir(dir_path)`: Creating list of file path to json in a given directory\n",
        "- `json_load(file_path)`: Getting JSON of given file path\n",
        "- `json_tokenization(data, fields, tokenizer)`: Creating a list of tokenized json data by list of json data\n",
        "- `tokens_to_vectors(inputs, model)`: Creating a list of embeddings by a list of tokenized json\n",
        "- `faiss_index_gen(embeddings, index_dir_path)`"
      ],
      "metadata": {
        "id": "TflCbMU5s6VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pymongo\n",
        "from pymongo import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "\n",
        "mongo_db_pass = userdata.get('mong_cluster_pass')\n",
        "url = f\"mongodb+srv://dndyd0206:{mongo_db_pass}@ha-rag-meta.nd2p6.mongodb.net/?retryWrites=true&w=majority&appName=HA-RAG-META\"\n",
        "\n",
        "client = MongoClient(url, server_api=ServerApi('1'))\n",
        "\n",
        "db = client['HA-RAG-META']\n",
        "collection = db['META']"
      ],
      "metadata": {
        "id": "us7umgOtrPT1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Getting\n",
        "JSON_DIR_PATH = '/content/drive/MyDrive/RAG_JSON_EMBEDDINGS_INDEX'\n",
        "HA_RAG_DATA_DIR_PATH = os.path.join(JSON_DIR_PATH, 'JSON/HA_RAG_DATA')\n",
        "HA_RAG_INDEX_DIR_PATH = os.path.join(JSON_DIR_PATH, 'INDEX/HA_RAG_INDEX')\n",
        "HA_RAG_EMBEDDINGS_DIR_PATH = os.path.join(JSON_DIR_PATH, 'EMBEDDINGS/HA_RAG_EMBEDDINGS')\n",
        "\n",
        "if not os.path.exists(HA_RAG_DATA_DIR_PATH):\n",
        "  os.makedirs(HA_RAG_DATA_DIR_PATH)\n",
        "\n",
        "if not os.path.exists(HA_RAG_INDEX_DIR_PATH):\n",
        "  os.makedirs(HA_RAG_INDEX_DIR_PATH)\n",
        "\n",
        "if not os.path.exists(HA_RAG_EMBEDDINGS_DIR_PATH):\n",
        "  os.makedirs(HA_RAG_EMBEDDINGS_DIR_PATH)\n",
        "\n",
        "json_files_path = json_dir(HA_RAG_DATA_DIR_PATH)\n",
        "\n",
        "print(json_files_path)\n",
        "\n",
        "## json_tokenizations & embeddings imports\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "inputs = None\n",
        "embeddings = None\n",
        "index = None\n",
        "for json_file_path in json_files_path:\n",
        "  print(\"\")\n",
        "  print(f\"Processing {json_file_path}\")\n",
        "  current_data = json_load(json_file_path)\n",
        "  print(\"\")\n",
        "  print(f\"Data length: {len(current_data)}\")\n",
        "  print(\"\")\n",
        "  print(f\"Tokenizing.....\")\n",
        "  inputs = json_tokenization(current_data, tokenizer)\n",
        "  print(\"\")\n",
        "  print(f\"Embedding.....\")\n",
        "  embeddings = tokens_to_vectors(inputs, model)\n",
        "  print(\"\")\n",
        "  print(\"Saving......\")\n",
        "  index = faiss_index_gen(\n",
        "      embeddings,\n",
        "      os.path.join(HA_RAG_INDEX_DIR_PATH, json_file_path.split(\"/\")[-1].split(\".\")[0]+\".index\"),\n",
        "      os.path.join(HA_RAG_EMBEDDINGS_DIR_PATH, json_file_path.split(\"/\")[-1].split(\".\")[0]+\".npy\"))\n",
        "  print(\"\")\n",
        "  print(\"Index, Embeddings and Tokenizing JSON DONE!\")\n",
        "  print(\"\")\n",
        "  print(\"Creating index field for json data and saving it\")\n",
        "  for i, record in enumerate(current_data):\n",
        "    record['index'] = i\n",
        "    collection.insert_one(record)\n",
        "  print(\"\")\n",
        "  print(\"Done\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GpjQhUGLcz8Y",
        "outputId": "3e56ab1e-e594-470e-9205-97a800f367e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['/content/drive/MyDrive/RAG_JSON_EMBEDDINGS_INDEX/JSON/HA_RAG_DATA/combined_common.json']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/RAG_JSON_EMBEDDINGS_INDEX/JSON/HA_RAG_DATA/combined_common.json\n",
            "Data length: 10296\n",
            "Tokenizing.....\n",
            "Embedding.....\n",
            "Saving......\n",
            "Index, Embeddings and Tokenizing JSON DONE!\n",
            "Creating index field for json data and saving it\n",
            "Done\n"
          ]
        }
      ]
    }
  ]
}