{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Env Setting\n",
        "- Python Virtual Env\n",
        "  - .mvp_rag_db_env"
      ],
      "metadata": {
        "id": "wkrYG3gddnsk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dK6OCcp9c6-d",
        "outputId": "b9501cc8-9ebc-4954-8bd6-4bdbd58a0956"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# importing google drive for the session\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating a virtual env just once\n",
        "# %cd /content/drive/MyDrive\n",
        "# !apt install python3.10-venv\n",
        "# !python3 -m venv .mvp_rag_db_env"
      ],
      "metadata": {
        "collapsed": true,
        "id": "IGWTEyvgfTcU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate the env\n",
        "!source /content/drive/MyDrive/.mvp_rag_db_env/bin/activate"
      ],
      "metadata": {
        "id": "SziovEVOf0gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Package Installation\n",
        "- torch\n",
        "- transformers\n",
        "- faiss-cpu\n",
        "- numpy\n",
        "- pandas\n",
        "- langchain\n",
        "- sqlalchemy"
      ],
      "metadata": {
        "id": "7u4zksYdf7qx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers faiss-cpu numpy pandas langchain sqlalchemy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "mfgOwuHogCpp",
        "outputId": "da29cfc8-2fdd-4fb7-a0d6-cbdcb2dda8b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.1+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.42.4)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.2.14)\n",
            "Requirement already satisfied: sqlalchemy in /usr/local/lib/python3.10/dist-packages (2.0.32)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /usr/local/lib/python3.10/dist-packages (from torch) (2.20.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.1)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.6.20)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Requirement already satisfied: langchain-core<0.3.0,>=0.2.32 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.33)\n",
            "Requirement already satisfied: langchain-text-splitters<0.3.0,>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.2.2)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.100)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.8.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (8.5.0)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy) (3.0.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.3.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.4)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3.0,>=0.2.32->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.7)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain) (2.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3.0,>=0.2.32->langchain) (3.0.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JSON Prep\n",
        "1. Load Json\n",
        "  - `json_dir(dir_path)`: Creating list of file path to json in a given directory\n",
        "  - `json_load(file_path)`: Getting JSON of given file path"
      ],
      "metadata": {
        "id": "b9t9AFVNpMxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Json\n",
        "import json\n",
        "import os\n",
        "\n",
        "def json_dir(dir_path):\n",
        "  json_files = os.listdir(dir_path)\n",
        "  json_files_path = [os.path.join(dir_path, file) for file in json_files]\n",
        "  return json_files_path\n",
        "\n",
        "\n",
        "def json_load(file_path):\n",
        "  current_data = None\n",
        "  if (os.path.exists(file_path)):\n",
        "    with open(file_path, 'r') as f:\n",
        "      current_data = json.load(f)\n",
        "  else:\n",
        "    print(f\"Can't find {file_path}\")\n",
        "  return current_data"
      ],
      "metadata": {
        "id": "qcsQs2-Epv12"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizing & Embedding: KoBert\n",
        "1. Load KoBert: Embedding Model and Tokenizer\n",
        "2. Json to Vectors\n",
        "  - Tokenization: json to tokenized text\n",
        "    - `json_tokenization(data, fields, tokenizer)`: Creating a list of tokenized json data by list of json data\n",
        "  - Embeddings Generation: tokenized json to vector\n",
        "    - `tokens_to_vectors(inputs, model)`: Creating a list of embeddings by a list of tokenized json"
      ],
      "metadata": {
        "id": "dnhoRyVKpUBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# KoBert Tokenization Installed\n",
        "!pip install 'git+https://github.com/SKTBrain/KoBERT.git#egg=kobert_tokenizer&subdirectory=kobert_hf'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ch3a39ouWNES",
        "outputId": "07c00bc4-991d-4515-e781-fd1e84e09bea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting kobert_tokenizer\n",
            "  Cloning https://github.com/SKTBrain/KoBERT.git to /tmp/pip-install-fyz5e92s/kobert-tokenizer_798163efbc9f410c8e0d237e3108dda3\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/SKTBrain/KoBERT.git /tmp/pip-install-fyz5e92s/kobert-tokenizer_798163efbc9f410c8e0d237e3108dda3\n",
            "  Resolved https://github.com/SKTBrain/KoBERT.git to commit 47a69af87928fc24e20f571fe10c3cc9dd9af9a3\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load KoBert: Embedding Model and Tokenizer\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1').to('cuda')\n",
        "model_cpu = BertModel.from_pretrained('skt/kobert-base-v1')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRP0yLFa0dbT",
        "outputId": "40357963-9905-4579-82ad-40bedf8334d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Json to Vectors\n",
        "# Tokenization: json to tokenized text\n",
        "def json_tokenization(data, tokenizer):\n",
        "  inputs = []\n",
        "  for row in data:\n",
        "      text = f\"{row['ad_gu']} [SEP] {row['ad_dong']} [SEP] {row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "               f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "               f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "      text_clean = text.replace('\\n', ' ')\n",
        "      # Tokenize and move input tensors to GPU\n",
        "      tokenized_input = tokenizer(text_clean, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "      tokenized_input = {key: value.to('cuda') for key, value in tokenized_input.items()}  # Move inputs to GPU\n",
        "      inputs.append(tokenized_input)\n",
        "  return inputs\n",
        "\n",
        "def json_tokenization_cpu(data, tokenizer):\n",
        "  inputs = []\n",
        "  for row in data:\n",
        "      text = f\"{row['ad_gu']} [SEP] {row['ad_dong']} [SEP] {row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "               f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "               f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "      text_clean = text.replace('\\n', ' ')\n",
        "      tokenized_input = tokenizer(text_clean, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
        "      inputs.append(tokenized_input)\n",
        "  return inputs\n",
        "\n",
        "# Embeddings Generation: tokenized json to vector\n",
        "def tokens_to_vectors(inputs, model):\n",
        "  model.to('cuda')\n",
        "  embeddings = []\n",
        "  for input in inputs:\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**input)\n",
        "      embedding = outputs.last_hidden_state[:, 0, :]\n",
        "      embedding = embedding.squeeze().cpu().numpy()\n",
        "      embeddings.append(embedding)\n",
        "  return embeddings\n",
        "\n",
        "def tokens_to_vectors_cpu(inputs, model):\n",
        "  embeddings = []\n",
        "  for input in inputs:\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**input)\n",
        "      embedding = outputs.last_hidden_state[:, 0, :]\n",
        "      embedding = embedding.squeeze().numpy()\n",
        "      embeddings.append(embedding)\n",
        "  return embeddings"
      ],
      "metadata": {
        "id": "5G03pXPi1wJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FAISS for Vector Search\n",
        "- FAISS creating an index for vectors\n",
        "  - `faiss_index_gen(embeddings, index_dir_path)`: Creating a index file of given embeddings\n",
        "- RDBMS metadata -> not now"
      ],
      "metadata": {
        "id": "APf6PeRcpfIl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "\n",
        "def faiss_index_gen(embeddings, index_dir_path):\n",
        "  embedding_dim = len(embeddings[0])\n",
        "  index = faiss.IndexFlatL2(embedding_dim)\n",
        "  embeddings_array = np.array(embeddings).astype('float32')\n",
        "  index.add(embeddings_array)\n",
        "  faiss.write_index(index, index_dir_path)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zq7HTpDEff8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Json to Vector DB\n",
        "- `json_dir(dir_path)`: Creating list of file path to json in a given directory\n",
        "- `json_load(file_path)`: Getting JSON of given file path\n",
        "- `json_tokenization(data, fields, tokenizer)`: Creating a list of tokenized json data by list of json data\n",
        "- `tokens_to_vectors(inputs, model)`: Creating a list of embeddings by a list of tokenized json\n",
        "- `faiss_index_gen(embeddings, index_dir_path)`"
      ],
      "metadata": {
        "id": "TflCbMU5s6VY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Variables\n",
        "## json_dir, json_load\n",
        "JSON_DIR_PATH = '/content/drive/MyDrive/JSON'\n",
        "HA_RAG_DATA_DIR_PATH = os.path.join(JSON_DIR_PATH, 'HA_RAG_DATA')\n",
        "HA_RAG_INDEX_DIR_PATH = os.path.join(JSON_DIR_PATH, 'HA_RAG_INDEX')\n",
        "if not os.path.exists(HA_RAG_DATA_DIR_PATH):\n",
        "  os.makedirs(HA_RAG_DATA_DIR_PATH)\n",
        "\n",
        "if not os.path.exists(HA_RAG_INDEX_DIR_PATH):\n",
        "  os.makedirs(HA_RAG_INDEX_DIR_PATH)\n",
        "\n",
        "json_files_path = json_dir(HA_RAG_DATA_DIR_PATH)\n",
        "\n",
        "## json_tokenizations\n",
        "from kobert_tokenizer import KoBERTTokenizer\n",
        "from transformers import BertModel\n",
        "\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "tokenizer = KoBERTTokenizer.from_pretrained('skt/kobert-base-v1')\n",
        "model = BertModel.from_pretrained('skt/kobert-base-v1')\n",
        "\n",
        "inputs = None\n",
        "embeddings = None\n",
        "for json_file_path in json_files_path:\n",
        "  print(f\"Processing {json_file_path}\")\n",
        "  current_data = json_load(json_file_path)\n",
        "  print(f\"Data length: {len(current_data)}\")\n",
        "  print(f\"Tokenizing.....\")\n",
        "  inputs = json_tokenization(current_data, tokenizer)\n",
        "  print(f\"Embedding.....\")\n",
        "  embeddings = tokens_to_vectors(inputs, model)\n",
        "  print(\"Saving......\")\n",
        "  faiss_index_gen(embeddings, os.path.join(HA_RAG_INDEX_DIR_PATH, json_file_path.split(\"/\")[-1].split(\".\")[0]+\".index\"))\n",
        "  print(\"Done\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BPfD1Eas6BE",
        "outputId": "e6d6e13a-4147-473d-bc35-331b0dfdcaff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'XLNetTokenizer'. \n",
            "The class this function is called from is 'KoBERTTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing /content/drive/MyDrive/JSON/HA_RAG_DATA/SeongBuk_common.json\n",
            "Data length: 990\n",
            "Tokenizing.....\n",
            "Embedding.....\n",
            "Saving......\n",
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing with LangChain"
      ],
      "metadata": {
        "id": "_m5S72f2pkfz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "collapsed": true,
        "id": "cceFhmM8Hj6_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "id": "5EGEPoHfGo0B",
        "outputId": "50ee8e55-27e3-4178-b52d-fec7680c869a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-c08ff71e8c29>\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtest_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfaiss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mvector_store\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFAISS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# LLM Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: FAISS.__init__() missing 2 required positional arguments: 'docstore' and 'index_to_docstore_id'"
          ]
        }
      ]
    }
  ]
}