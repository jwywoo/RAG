{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Colab Imports"
      ],
      "metadata": {
        "id": "sBLGI9O7gs9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VXcQXX-Cgsa1",
        "outputId": "684a649a-955c-4dd7-effd-7bd81b33758a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installations"
      ],
      "metadata": {
        "id": "Kot42h8ag6aG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2HGOQLyOn0lg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "581734fc-84e2-47ef-b90e-6888f169a053"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m47.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.6/49.6 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.2/292.2 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.5/383.5 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -qU langchain-community faiss-cpu langchain-openai tiktoken"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Json loading and directory path set"
      ],
      "metadata": {
        "id": "B4CbEggdg8cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "JSON_DIR_PATH = '/content/drive/MyDrive/RAG_JSON_EMBEDDINGS_INDEX'\n",
        "HA_RAG_DATA_PATH = os.path.join(JSON_DIR_PATH, \"JSON/HA_RAG_DATA\")\n",
        "if not os.path.exists(HA_RAG_DATA_PATH):\n",
        "    os.makedirs(HA_RAG_DATA_PATH)\n",
        "\n",
        "json_path = os.path.join(HA_RAG_DATA_PATH, \"combined_common.json\")\n",
        "\n",
        "with open(os.path.join(HA_RAG_DATA_PATH, json_path), \"r\") as f:\n",
        "        data = json.load(f)"
      ],
      "metadata": {
        "id": "zddk14ighYtI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index Path(Directory to save initialized index)\n",
        "DRIVE_PATH = '/content/drive/MyDrive/RAG_JSON_EMBEDDINGS_INDEX'\n",
        "INDEX_DIR_PATH = os.path.join(DRIVE_PATH, \"INDEX\")\n",
        "HA_INDEX_PATH = os.path.join(INDEX_DIR_PATH, \"HA_TEST_INDEX\")\n",
        "if not os.path.exists(INDEX_DIR_PATH):\n",
        "    os.makedirs(INDEX_DIR_PATH)\n",
        "\n",
        "if not os.path.exists(HA_INDEX_PATH):\n",
        "    os.makedirs(HA_INDEX_PATH)\n",
        "\n",
        "faiss_index_path = os.path.join(HA_INDEX_PATH, \"test_faiss_index\")"
      ],
      "metadata": {
        "id": "bGM6yF8zhgsj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Less Tokens: Meta Data"
      ],
      "metadata": {
        "id": "8o8CEzfuilfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_documents_meta = []\n",
        "import uuid\n",
        "from uuid import uuid4\n",
        "from langchain.schema import Document\n",
        "\n",
        "for row in data:\n",
        "  # Text\n",
        "  text = f\"{row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "  f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "  f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "  # Metadata\n",
        "  metadata = {\n",
        "      \"ad_gu\": row['ad_gu'],\n",
        "      \"ad_dong\": row['ad_dong'],\n",
        "  }\n",
        "  clean_text = text.replace(\"\\n\", \" \")\n",
        "  test_documents_meta.append(Document(\n",
        "      page_content=clean_text,\n",
        "      metadata=metadata\n",
        "  ))\n",
        "\n",
        "# Only for Faiss -> comment it out for Pinecone\n",
        "uuids = [str(uuid4()) for _ in range(len(test_documents_meta))]"
      ],
      "metadata": {
        "id": "PkQENiPKl7jS"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_docuemtns_no_meta = []\n",
        "import uuid\n",
        "from uuid import uuid4\n",
        "from langchain.schema import Document\n",
        "\n",
        "for row in data:\n",
        "  text = f\"{row['ad_gu']} [SEP] {row['ad_dong']} [SEP] {row['address']} [SEP] {row['location']} [SEP] \" \\\n",
        "               f\"{row['description']} [SEP] {row['rating']} [SEP] {row['share_link']} [SEP] \" \\\n",
        "               f\"{' '.join(row['reviews'])} [SEP] {row['info']}\"\n",
        "  clean_text = text.replace(\"\\n\", \" \")\n",
        "  test_docuemtns_no_meta.append(Document(page_content=clean_text))\n",
        "\n",
        "# Only for Faiss -> comment it out for Pinecone\n",
        "uuids = [str(uuid4()) for _ in range(len(test_docuemtns_no_meta))]"
      ],
      "metadata": {
        "id": "qhK55ezFxMTj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "tokenizer = tiktoken.encoding_for_model(\"text-embedding-3-small\")\n",
        "\n",
        "no_meta_avg_token_num = 0\n",
        "meta_avg_token_num = 0\n",
        "\n",
        "for i in range(100):\n",
        "  no_meta_avg_token_num += len(tokenizer.encode(test_docuemtns_no_meta[i].page_content))\n",
        "  meta_avg_token_num += len(tokenizer.encode(test_documents_meta[i].page_content))\n",
        "\n",
        "no_meta_avg_token_num /= 100\n",
        "meta_avg_token_num /= 100\n",
        "print(f\"No Meta Avg Token Num: {no_meta_avg_token_num}\")\n",
        "print(f\"Meta Avg Token Num: {meta_avg_token_num}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3jWHV-UnOR4",
        "outputId": "61b97ad8-b9b5-4317-8b83-42f8c82ddb0a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No Meta Avg Token Num: 1057.13\n",
            "Meta Avg Token Num: 1042.13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store Init"
      ],
      "metadata": {
        "id": "jB_XNbEJx-BR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# FAISS init\n",
        "import faiss\n",
        "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "# Embedding Model Selection\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('openAI')\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "# Index Dimension\n",
        "index_cpu = faiss.IndexFlatL2(len(embeddings.embed_query(test_documents_meta[0].page_content)))\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index_cpu,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ],
      "metadata": {
        "id": "-reDgsLhmcF_"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Index Dimension\n",
        "index_cpu = faiss.IndexFlatL2(len(embeddings.embed_query(test_documents_meta[0].page_content)))\n",
        "\n",
        "new_vector_store = FAISS(\n",
        "    embedding_function=embeddings,\n",
        "    index=index_cpu,\n",
        "    docstore=InMemoryDocstore(),\n",
        "    index_to_docstore_id={},\n",
        ")"
      ],
      "metadata": {
        "id": "se7cqCbk7Ck3"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch\n",
        "- Splitting Docuements into multiple batches\n",
        "- Append new Documents into existing vector store"
      ],
      "metadata": {
        "id": "dfv9u50hMr_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting Documents"
      ],
      "metadata": {
        "id": "XCNxkNQMOPtV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 700\n",
        "batches = [test_documents_meta[i:i+batch_size] for i in range(0, len(test_documents_meta), batch_size)]\n",
        "batches_id = [uuids[i:i+batch_size] for i in range(0, len(uuids), batch_size)]\n",
        "print(f\"Number of documents: {len(test_documents_meta)}\")\n",
        "print(f\"Batch Size(Number of documents in one batch): {batch_size}\")\n",
        "print(\"Expected TPM: %.2f\"% (batch_size*meta_avg_token_num))\n",
        "print(f\"Number of batches: {len(batches)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5irYK0VJOPeQ",
        "outputId": "16d41580-fa05-4392-a58a-48cdeda3673b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 21164\n",
            "Batch Size(Number of documents in one batch): 700\n",
            "Expected TPM: 729491.00\n",
            "Number of batches: 31\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vector Store Merging"
      ],
      "metadata": {
        "id": "cGskHciGGt1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_store.index.ntotal)\n",
        "print(new_vector_store.index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo61Y25wFXeJ",
        "outputId": "59afc44f-343e-4211-ec4b-d8e395a4f959"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.add_documents(documents=batches[0], ids=batches_id[0])\n",
        "print(vector_store.index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtjJm--75oND",
        "outputId": "3f97b63d-6d5c-42af-a5fd-9f7b616ec74b"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.add_documents(documents=batches[1], ids=batches_id[1])\n",
        "print(vector_store.index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xlPEArRL6Zx6",
        "outputId": "943d9ff7-364f-4302-bb99-b1eb4b70f2a0"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_vector_store.add_documents(documents=batches[2], ids=batches_id[2])\n",
        "print(new_vector_store.index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I4S8LuT97Hr7",
        "outputId": "36172a5f-11b7-4ef0-e78e-f3004f69837f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "700\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vector_store.merge_from(new_vector_store)\n",
        "print(vector_store.index.ntotal)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtoI3emn6oB8",
        "outputId": "3a2b72c8-be72-4d05-9858-0039f9c137f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Duplication Check & append"
      ],
      "metadata": {
        "id": "b5yLwsOiG0sl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "\n",
        "# Batch 순회\n",
        "existing_texts = {doc.page_content for doc in vector_store.docstore._dict.values()}\n",
        "for i in range(len(batches)):\n",
        "  print(f\"Batch {i+1}/{len(batches)}\")\n",
        "  checked_batch = []\n",
        "  checked_batch_id = []\n",
        "  dup_count = 0\n",
        "  # Batch duplication check with original dataset or saved data\n",
        "  for j in range(len(batches[i])):\n",
        "    if batches[i][j].page_content not in existing_texts:\n",
        "      checked_batch.append(batches[i][j])\n",
        "      checked_batch_id.append(batches_id[i][j])\n",
        "    else:\n",
        "      dup_count+=1\n",
        "  print(\"Duplicate Found: {}\".format(dup_count))\n",
        "  print(\"New Embeddings: {}\".format(len(checked_batch)))\n",
        "  if (len(checked_batch) == 0):\n",
        "    continue\n",
        "  vector_store.add_documents(documents=checked_batch, ids=checked_batch_id)\n",
        "  print(\"Current Vector Store Size: \", vector_store.index.ntotal)\n",
        "  sleep(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRUeLP3MNE1R",
        "outputId": "d292a57f-2f4d-4742-cac3-e24b81c5454c"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1/31\n",
            "Duplicate Found: 700\n",
            "New Embeddings: 0\n",
            "Batch 2/31\n",
            "Duplicate Found: 700\n",
            "New Embeddings: 0\n",
            "Batch 3/31\n",
            "Duplicate Found: 700\n",
            "New Embeddings: 0\n",
            "Batch 4/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 5/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 6/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 7/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 8/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 9/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 10/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 11/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 12/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 13/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 14/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 15/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 16/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 17/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 18/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 19/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 20/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 21/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 22/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 23/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 24/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 25/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 26/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 27/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 28/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 29/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 30/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 700\n",
            "Batch 31/31\n",
            "Duplicate Found: 0\n",
            "New Embeddings: 164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(vector_store.index.ntotal)\n",
        "vector_store.save_local(faiss_index_path)"
      ],
      "metadata": {
        "id": "G96Qsw4X14f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "602c6464-f8ff-4210-8333-dc24c99e9ab0"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "21164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Optimized Vector Store Data Ingestion"
      ],
      "metadata": {
        "id": "4iYmYzrgdVOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "\n",
        "# Batch 순회\n",
        "existing_texts = {doc.page_content for doc in vector_store.docstore._dict.values()}\n",
        "for i in range(len(batches)):\n",
        "  print(f\"Batch {i+1}/{len(batches)}\")\n",
        "  checked_batch = []\n",
        "  checked_batch_id = []\n",
        "  dup_count = 0\n",
        "  # Batch duplication check with original dataset or saved data\n",
        "  for j in range(len(batches[i])):\n",
        "    if batches[i][j].page_content not in existing_texts:\n",
        "      checked_batch.append(batches[i][j])\n",
        "      checked_batch_id.append(batches_id[i][j])\n",
        "    else:\n",
        "      dup_count+=1\n",
        "  print(\"Duplicate Found: {}\".format(dup_count))\n",
        "  print(\"New Embeddings: {}\".format(len(checked_batch)))\n",
        "\n",
        "  if (len(checked_batch) == 0):\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    vector_store.add_documents(documents=checked_batch, ids=checked_batch_id)\n",
        "    print(\"Current Vector Store Size: \", vector_store.index.ntotal)\n",
        "    sleep(5)\n",
        "  except Exception as e:\n",
        "    vector_store.save_local(faiss_index_path)\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "QDtBVKrPdUz4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}